# ğŸšš CARGO: Confidence-Aware Routing of Large Language Models

**CARGO** (**Category-Aware Routing with Gap-based Optimization**) is a lightweight, **confidence-aware router** for large language models (LLMs). It selects the best expert model per prompt using a two-stage strategy:

1. A **regression model** scores LLM responses using prompt embeddings.  
2. If the **top two scores are too close**, a **classifier** breaks the tie.  

CARGO achieves **up to 86% accuracy**, outperforming individual LLMs across coding, math, reasoning, and summarization tasks.

---

## ğŸ“¦ Features

- âœ… Embedding-based regression scoring  
- ğŸ§  Binary classifier fallback for tie-breaking  
- ğŸ—ƒ Supports global and category-specific models  
- ğŸ§ª No human labels required â€” uses LLM-as-a-judge for annotation  
- ğŸ”Œ Easily extendable to new domains or LLMs  

---

## ğŸ” Prerequisites

1. **Python â‰¥ 3.8**  
2. Install dependencies:
```
   pip install -r requirements.txt
```

3. (Optional) If collecting new responses, set your [OpenRouter](https://openrouter.ai) API key:

   ```
   export OPENROUTER_API_KEY="your-api-key"
   ```

---

## ğŸ§­ Pipeline Overview

```mermaid
flowchart TD
    A[Prompt Dataset] --> B[LLM Response Collection]
    B --> C[Pairwise LLM Annotation]
    C --> D[Ranked CSV]
    D --> E[Ranked Binary CSV]
    E+D --> F1[CARGO per Category]
    E+D --> F2[CARGO Global]
```

---

## ğŸ“‚ Step-by-Step Workflow

---

### ğŸ“‚ A. Prompt Dataset (**Optional**)

Scripts:

* `Prompt Response Collection/response_collection.py`
* `Prompt Response Collection/merge_responses.py`

**Purpose:**

* Collect responses from multiple LLMs using prompts.
* Merge and deduplicate batches into one dataset.

**Output:**

* `Full Dataset Ranking/Full_dataset_with_id.csv`

---

### ğŸ“¦ B. LLM Response Collection

ğŸ“ **Input file for pipeline start:**

* `Full Dataset Ranking/Full_dataset_with_id.csv`

This file contains all collected responses per prompt and is the entry point for ranking.

**Format:**

```csv
prompt_id,category,model,response
1,math,gpt-4,"The integral of x is (1/2)xÂ² + C"
1,math,claude-2,"Integrating x gives (1/2)xÂ² plus a constant."
```

---

### ğŸ† C. Pairwise LLM Annotation â†’ Ranked CSV

Script:

* `Full Dataset Ranking/rank_script_V3.py`

**What it does:**

* Compares multiple responses per prompt.
* Generates rankings using pairwise wins/losses, heuristics, or LLM-as-a-judge scoring.

**Output:**

* `Full Dataset Ranking/ranked_responses_final.csv`

---

### âœ… D. Ranked CSV

ğŸ“ File:

* `Full Dataset Ranking/ranked_responses_final.csv`
  (also duplicated in `Training/ranked_responses_final.csv`)

This is the **training supervision file** for regression/classification models.

---

### ğŸ§¬ E. Ranked Binary CSV (Pairwise Format)

Notebook:

* `Training/Binary Dataet.ipynb`

**Inputs:**

* `Full Dataset Ranking/ranked_responses_final.csv`
* `Training/df_processed.pkl` (optional metadata file)

**What it does:**

* Builds all response pairs per prompt.
* Labels `1` if A ranked higher than B, else `0`.

**Output:**

* `Training/df_pairwise_v2.pkl`

**Format (pairwise rows):**

```pkl
{
  prompt_id: 1,
  model_A: "gpt-4",
  model_B: "claude-2",
  label: 1   # A wins
}
```

---

### ğŸ—ï¸ F1. CARGO per Category (Regressor)

Notebook:

* `Training/train_category_NEW.ipynb`

**Inputs:**

* `Training/df_pairwise_v2.pkl`
* `Training/ranked_responses_final.csv`

**Process:**

* Train regression models per category (Math, Code, Reasoning, English, Summaries).
* Apply Ï„-gap threshold logic.
* Evaluate and plot accuracy vs Ï„.

---

### ğŸ§ª F2. CARGO Global (Regressor)

Notebook:

* `Training/train_global_Regressor.ipynb`

**Inputs:**

* `Training/df_pairwise_v2.pkl`
* `Training/ranked_responses_final.csv`

**Process:**

* Train a single global regressor across all categories.
* Apply Ï„-gap threshold logic.
* Evaluate global performance and plot results.

---

## ğŸ“Š Scalability Experiments

Validation datasets with human labels are provided for benchmarking:

* `Scalability Experiments/250_human_label.csv`
* `Scalability Experiments/experiment1_human_label.csv`
* `Scalability Experiments/experiment2_human_label.csv`

These allow evaluation of routing accuracy against human judgments.

---

## ğŸ“ Repository Structure

```
C:.
â”‚   README.md
â”‚   requirements.txt
â”‚
â”œâ”€â”€â”€Full Dataset Ranking
â”‚       Full_dataset_with_id.csv
â”‚       ranked_responses_final.csv
â”‚       rank_script_V3.py
â”‚
â”œâ”€â”€â”€Prompt Response Collection
â”‚       merge_responses.py
â”‚       response_collection.py
â”‚
â”œâ”€â”€â”€Scalability Experiments
â”‚       250_human_label.csv
â”‚       experiment1_human_label.csv
â”‚       experiment2_human_label.csv
â”‚
â””â”€â”€â”€Training
        Binary Dataet.ipynb
        df_pairwise.pkl
        df_pairwise_v2.pkl
        ranked_responses_final.csv
        train_category_NEW.ipynb
        train_global_Regressor.ipynb
```

---

## ğŸš€ Extending CARGO

* Add new LLMs by updating the response collection script.
* Add new categories for specialized regressors.
* Tune Ï„-thresholds for different confidence levels.

---

## ğŸ‘¨â€ğŸ”¬ Citation

If you use CARGO in your work, please cite:
```
@inproceedings{barrak2025cargo,
  title     = {CARGO: A Framework for Confidence-Aware Routing of Large Language Models},
  author    = {Barrak, Amine and Fourati, Yosr and Olchawa, Michael and Ksontini, Emna and Zoghlami, Khalil},
  booktitle = {Proceedings of the International Conference on Collaborative Advances in Software and COmputiNg (CASCON)},
  year      = {2025},
  pages     = {1--10}
}
```


